{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "from collections import deque\n",
    "from gymnasium.wrappers import FrameStack\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants and hyperparameters\n",
    "num_episodes = 100\n",
    "max_steps_per_episode = 1000\n",
    "learning_rate = 0.01 # Use 0.0001 for 10000 episodes, 0.001 for 1000 episodes, and 0.01 for 100 episodes\n",
    "batch_size = 64\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon = 1.0  # Exploration rate\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.98 # Use 0.995 for 1000 episodes, 0.98 for 100 episodes\n",
    "memory = deque(maxlen=10000)  # Experience replay buffer\n",
    "env_name = \"ALE/Frogger-v5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, num_actions):\n",
    "    model = Sequential([ #Used half of the initial filter amounts, 512 -> 256, 256 -> 128, 64 -> 32, 32 -> 16 \n",
    "        Conv2D(16, kernel_size=(8, 8), strides=(4, 4), activation='relu', input_shape=input_shape, data_format=\"channels_first\"),\n",
    "        Conv2D(32, kernel_size=(4, 4), strides=(2, 2), activation='relu'),\n",
    "        Conv2D(32, kernel_size=(3, 3), strides=(1, 1), activation='relu'),\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(num_actions, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = gym.make(env_name, obs_type='grayscale', render_mode=None)\n",
    "num_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FrameStack(env, 4)\n",
    "frames, width, height = env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 210, 160)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the DQN model\n",
    "model = build_model((frames, width, height), num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode Progress:   1%|          | 1/100 [00:22<37:35, 22.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Previous episode: Episode: 1/100, Total Reward: 9.0, Epsilon: 0.7847"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode Progress:   2%|▏         | 2/100 [00:44<36:27, 22.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Previous episode: Episode: 2/100, Total Reward: 9.0, Epsilon: 0.7690"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode Progress:   3%|▎         | 3/100 [01:08<37:20, 23.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Previous episode: Episode: 3/100, Total Reward: 16.0, Epsilon: 0.7536"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode Progress:   4%|▍         | 4/100 [01:30<36:19, 22.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Previous episode: Episode: 4/100, Total Reward: 9.0, Epsilon: 0.7386"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode Progress:   5%|▌         | 5/100 [01:57<38:19, 24.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Previous episode: Episode: 5/100, Total Reward: 8.0, Epsilon: 0.7238"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode Progress:   6%|▌         | 6/100 [02:24<39:07, 24.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Previous episode: Episode: 6/100, Total Reward: 9.0, Epsilon: 0.7093"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for episode in tqdm(range(num_episodes), desc='Episode Progress', position=0):\n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        if np.random.rand() <= epsilon:\n",
    "            action = env.action_space.sample()  # Exploration\n",
    "        else:\n",
    "            q_values = model.predict(np.array([state]), verbose=None)[0]\n",
    "            action = np.argmax(q_values)  # Exploitation\n",
    "\n",
    "        # Ensure action is within bounds\n",
    "        action = np.clip(action, 0, num_actions - 1)\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        episode_reward += reward\n",
    "\n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Experience replay\n",
    "    if len(memory) >= batch_size:\n",
    "        minibatch = random.sample(memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + gamma * np.amax(model.predict(np.array([next_state]), verbose=None)[0])\n",
    "\n",
    "            target_f = model.predict(np.array([state]), verbose=None)\n",
    "            target_f[0][action] = target\n",
    "            model.fit(np.array([state]), target_f, epochs=1, verbose=None)\n",
    "\n",
    "    # Decay exploration rate\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "    print(f\"\\rPrevious episode: Episode: {episode + 1}/{num_episodes}, Total Reward: {episode_reward}, Epsilon: {epsilon:.4f}\", end=\"\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
