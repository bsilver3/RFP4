{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "712c3de9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2753010714.py, line 39)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\mnemoliaev\\AppData\\Local\\Temp\\ipykernel_6068\\2753010714.py\"\u001b[1;36m, line \u001b[1;32m39\u001b[0m\n\u001b[1;33m    def update(s, a, r, new_s)\u001b[0m\n\u001b[1;37m                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import random\n",
    "\n",
    "desc=[\"FFGFFF\", \"FHHHFH\", \"FHFFFH\", \"FFSFFH\", \"FFFHHH\", \"FFFFGH\"]\n",
    "env = gym.make('FrozenLake-v1', desc=desc, map_name=\"6x6\", is_slippery=False, render_mode=\"human\") \n",
    "observation, info = env.reset()\n",
    "\n",
    "Q = {}\n",
    "states = list(range(env.observation_space.n))\n",
    "actions = list(range(env.action_space.n))\n",
    "\n",
    "for s in states:\n",
    "    Q[s] = {}\n",
    "    for a in actions:\n",
    "        Q[s][a] = 0.0\n",
    "        \n",
    "learning_rate = .5\n",
    "num_episodes = 100\n",
    "\n",
    "def print_nonzero_q_values():\n",
    "    for s, actions in Q.items():\n",
    "        for a, value in actions.items():\n",
    "            if value != 0:\n",
    "                print(f\"Q[{s}][{a}] = {value}\")\n",
    "                \n",
    "def get_action(s):\n",
    "    if random.random() < .05:\n",
    "        return random.choice(actions)\n",
    "    \n",
    "    available_actions = Q[s]\n",
    "    \n",
    "    if all(v == 0 for v in available_actions.values()):\n",
    "        return random.choice(actions)\n",
    "\n",
    "    best_value = max(available_actions.values())\n",
    "    best_action = [a for a, v in available_actions.items() if v == best_value]\n",
    "    return best_action[0]\n",
    "\n",
    "def update(s, a, r, new_s):\n",
    "    maximum_future_reward = max(Q[new_s].values())\n",
    "    maximum_additional_reward = maximum_future_reward - Q[s][a]\n",
    "    new_reward = r + maximum_additional_reward\n",
    "    new_reward *= learning_rate\n",
    "    Q[s][a] += new_reward\n",
    "\n",
    "for episode_num in range(1, num_episodes+1):\n",
    "    observation, info = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        action = get_action(observation)\n",
    "        new_observation, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        if new_observation == observation:\n",
    "            continue\n",
    "        update(observation, action, reward, new_observation)\n",
    "    \n",
    "        if terminated or truncated:\n",
    "            if episode_num % 10 == 0:\n",
    "                print(f\"Ending episode {episode_num}/{num_episodes}\")\n",
    "            break \n",
    "\n",
    "    observation = new_observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8392077",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Box' object has no attribute 'n'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6068\\2480522702.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Initialize Q-table\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mQ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Q-learning parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Box' object has no attribute 'n'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('ALE/Frogger-v5')  # Assuming this is the correct import path\n",
    "\n",
    "# Initialize Q-table\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "# Q-learning parameters\n",
    "learning_rate = 0.5\n",
    "num_episodes = 100\n",
    "\n",
    "def get_action(s):\n",
    "    if np.random.rand() < 0.05:\n",
    "        return np.random.randint(env.action_space.n)  # explore\n",
    "    else:\n",
    "        return np.argmax(Q[s, :])  # exploit\n",
    "\n",
    "def update(s, a, r, new_s):\n",
    "    max_future_reward = np.max(Q[new_s, :])\n",
    "    Q[s, a] += learning_rate * (r + max_future_reward - Q[s, a])\n",
    "\n",
    "# Frogger game loop\n",
    "for episode_num in range(1, num_episodes+1):\n",
    "    observation = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        action = get_action(observation)\n",
    "        new_observation, reward, done, _ = env.step(action)\n",
    "        \n",
    "        if new_observation == observation:\n",
    "            continue\n",
    "        \n",
    "        update(observation, action, reward, new_observation)\n",
    "        observation = new_observation\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "# After training, you can use the Q-table to play the game\n",
    "observation = env.reset()\n",
    "while True:\n",
    "    action = np.argmax(Q[observation, :])\n",
    "    new_observation, _, done, _ = env.step(action)\n",
    "    observation = new_observation\n",
    "    if done:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39213bfc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6068\\3720611761.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_observation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mnew_observation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6068\\3720611761.py\u001b[0m in \u001b[0;36mpreprocess_observation\u001b[1;34m(observation)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mimage_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# Preprocess the image array as needed (e.g., resize, normalize)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mpreprocessed_observation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpreprocessed_observation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocess' is not defined"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('ALE/Frogger-v5')  # Assuming this is the correct import path\n",
    "\n",
    "# Initialize Q-table\n",
    "Q = np.zeros((env.action_space.n))\n",
    "\n",
    "# Q-learning parameters\n",
    "learning_rate = 0.5\n",
    "discount_factor = 0.99\n",
    "num_episodes = 100\n",
    "\n",
    "def preprocess_observation(observation):\n",
    "    # Assuming the observation is a tuple with an array and a dictionary\n",
    "    image_array, _ = observation\n",
    "    # Preprocess the image array as needed (e.g., resize, normalize)\n",
    "    preprocessed_observation = preprocess(image_array)\n",
    "    return preprocessed_observation\n",
    "\n",
    "def get_action(s):\n",
    "    if np.random.rand() < 0.05:\n",
    "        return np.random.randint(env.action_space.n)  # explore\n",
    "    else:\n",
    "        return np.argmax(Q)  # exploit\n",
    "\n",
    "for episode_num in range(1, num_episodes+1):\n",
    "    observation = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        state = preprocess_observation(observation)\n",
    "        action = get_action(state)\n",
    "        new_observation, reward, done, _ = env.step(action)\n",
    "        \n",
    "        if new_observation == observation:\n",
    "            continue\n",
    "        \n",
    "        new_state = preprocess_observation(new_observation)\n",
    "        best_future_reward = np.max(Q[new_state])\n",
    "        target = reward + discount_factor * best_future_reward\n",
    "        Q[state, action] += learning_rate * (target - Q[state, action])\n",
    "        \n",
    "        observation = new_observation\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "# After training, you can use the Q-table to play the game\n",
    "observation = env.reset()\n",
    "while True:\n",
    "    state = preprocess_observation(observation)\n",
    "    action = np.argmax(Q[state])\n",
    "    new_observation, _, done, _ = env.step(action)\n",
    "    observation = new_observation\n",
    "    if done:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab7fa0bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6068\\682232110.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mnew_observation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mnew_observation\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('ALE/Frogger-v5')  # Assuming this is the correct import path\n",
    "\n",
    "# Initialize Q-table\n",
    "Q = np.zeros((env.action_space.n))\n",
    "\n",
    "# Q-learning parameters\n",
    "learning_rate = 0.5\n",
    "discount_factor = 0.99\n",
    "num_episodes = 100\n",
    "\n",
    "def preprocess_observation(observation):\n",
    "    # Assuming the observation is a tuple with an array and a dictionary\n",
    "    image_array, _ = observation\n",
    "    # Preprocess the image array as needed (e.g., resize, normalize)\n",
    "    preprocessed_observation = image_array  # Placeholder for actual preprocessing\n",
    "    return preprocessed_observation\n",
    "\n",
    "def get_action(s):\n",
    "    if np.random.rand() < 0.05:\n",
    "        return np.random.randint(env.action_space.n)  # explore\n",
    "    else:\n",
    "        return np.argmax(Q)  # exploit\n",
    "\n",
    "for episode_num in range(1, num_episodes+1):\n",
    "    observation = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        state = preprocess_observation(observation)\n",
    "        action = get_action(state)\n",
    "        new_observation, reward, done, _, _ = env.step(action)\n",
    "        \n",
    "        if new_observation == observation:\n",
    "            continue\n",
    "        \n",
    "        new_state = preprocess_observation(new_observation)\n",
    "        best_future_reward = np.max(Q[new_state])\n",
    "        target = reward + discount_factor * best_future_reward\n",
    "        Q[state, action] += learning_rate * (target - Q[state, action])\n",
    "        \n",
    "        observation = new_observation\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "# After training, you can use the Q-table to play the game\n",
    "observation = env.reset()\n",
    "while True:\n",
    "    state = preprocess_observation(observation)\n",
    "    action = np.argmax(Q[state])\n",
    "    new_observation, _, done, _ = env.step(action)\n",
    "    observation = new_observation\n",
    "    if done:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578ddefb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
